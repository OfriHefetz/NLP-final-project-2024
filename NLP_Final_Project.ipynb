{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HABUWGDAb6H"
      },
      "source": [
        "# NLP Final Project 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TySf4I12Ai8J"
      },
      "source": [
        "## Introduction\n",
        "This research investigates if advanced language models can act as rational agents in simulated second-price auctions (the winner pays the second-highest bid) by strategically bidding their true value based on limited information, as economic theory predicts for rational agents.\n",
        "\n",
        "\n",
        "In this work, we will use the following open-source LLMs:\n",
        "\n",
        "1. Meta-Llama-3.1-8B - https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
        "2. Mistral -https://huggingface.co/mistralai/Mistral-7B-v0.1\n",
        "3. T5 - https://huggingface.co/google/flan-t5-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yC_q8RBIisEY"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install accelerate transformers\n",
        "!pip install -U flash-attn --no-build-isolation\n",
        "!pip install transformers bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjQdK3poAAoC"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline, AutoModelForCausalLM, AutoTokenizer,T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrPEeF9UtbHd"
      },
      "outputs": [],
      "source": [
        "# logging into huggingface\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoioBItdp-XE"
      },
      "source": [
        "### Promts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZkFGpfGRiYM"
      },
      "outputs": [],
      "source": [
        "# Convert test prompt to a single string\n",
        "def format_prompt(prompt_list):\n",
        "    formatted_prompt = \"\"\n",
        "    for prompt in prompt_list:\n",
        "        formatted_prompt += f\"{prompt['role']}: {prompt['content']}\\n\"\n",
        "    return formatted_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U7NHY_2qE1Q"
      },
      "outputs": [],
      "source": [
        "# Zero shot\n",
        "small_item_zero = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"Your value for the item is 1 USD. This is all the information you have (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "\n",
        "medium_item_zero = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participant is only exposed to his own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"you value from the item is 1000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info.(2) briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "big_item_zero = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participant is only exposed to his own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"you value from the item is 10,000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info.(2) briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "\n",
        "giant_item_zero = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participant is only exposed to his own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"you value from the item is 10,000,000,000,000,000,000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info.(2) briefly explain your reasoning.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNRVm6UqR3wa"
      },
      "outputs": [],
      "source": [
        "# Zero shot\n",
        "formatted_small_zero = format_prompt(small_item_zero)\n",
        "formatted_medium_zero = format_prompt(medium_item_zero)\n",
        "formatted_big_zero = format_prompt(big_item_zero)\n",
        "formatted_giant_zero = format_prompt(giant_item_zero)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One shot\n",
        "small_item_one = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"Your value for the item is 1 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I will bid 1 USD. My reasoning is that in a second-price auction, bidding your true valuation maximizes your expected utility, as you pay the second-highest bid if you win.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Your value for the item is 2 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "medium_item_one = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"You value the item at 1000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I will bid 1000 USD. My reasoning is that in a second-price auction, bidding your true valuation ensures that if you win, you pay a price lower than or equal to your valuation, maximizing your utility.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You value the item at 1500 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "big_item_one = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"You value the item at 10,000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I will bid 10,000 USD. My reasoning is that in a second-price auction, bidding your true valuation ensures that you maximize your utility by either winning at a favorable price or not winning if the price is too high.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You value the item at 15,000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "giant_item_one = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"You value the item at 10,000,000,000,000,000,000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I will bid 10,000,000,000,000,000,000 USD. My reasoning is that bidding your true valuation in a second-price auction ensures that you either win at a price you are willing to pay or do not win at all, which maximizes your expected utility.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You value the item at 15,000,000,000,000,000,000 USD. This is all the information you have. (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "YnYvyJADfQWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_small_one = format_prompt(small_item_one)\n",
        "formatted_medium_one = format_prompt(medium_item_one)\n",
        "formatted_big_one = format_prompt(big_item_one)\n",
        "formatted_giant_one = format_prompt(giant_item_one)"
      ],
      "metadata": {
        "id": "1AfzRkUNgN3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# items\n",
        "\n",
        "dental_floss = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \"The item: Plackers Micro Mint Dental Floss Picks with Travel Case, 12 Count. Your value for the item is 1 USD. This is all the information you have (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "\n",
        "speaker = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \" The item: Google Home White Slate One Size Smart Speaker. Your value for the item is 116 USD. This is all the information you have (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "\n",
        "rolex = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \" The item: Rolex Oyster Perpetual 41mm Automatic Blue Dial Men's Watch. Your value for the item is 9850 USD. This is all the information you have (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]\n",
        "\n",
        "spaceship = [\n",
        "    {\"role\": \"system\", \"content\": 'You are a bidder, participating in a second-price auction, where each bidder maximizes their expected utility by bidding their valuation of the item for sale. Each of the X participants is only exposed to their own bid.'},\n",
        "    {\"role\": \"user\", \"content\": \" The item: a large and impressive spaceship. Your value for the item is 10,000,000,000,000,000,000 USD. This is all the information you have (1) How much will you bid? Just return your bid and no other info. (2) Briefly explain your reasoning.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "_sX-NRiQgumm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dental_floss = format_prompt(dental_floss)\n",
        "formatted_speaker = format_prompt(speaker)\n",
        "formatted_rolex = format_prompt(rolex)\n",
        "formatted_spaceship = format_prompt(spaceship)"
      ],
      "metadata": {
        "id": "YubWQT-UiGzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J0DyugvZZr7"
      },
      "outputs": [],
      "source": [
        "N = 10 # Number of responses we are looking to get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzqrdZ1bp2GG",
        "outputId": "74e70edc-2534-49c2-8d9a-b38f2687854c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Ensure GPU is selected in Colab runtime\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfta3-fWDiBo"
      },
      "source": [
        "### *Mistral*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QIsltTU9_OuS"
      },
      "outputs": [],
      "source": [
        "# Path to offload directory\n",
        "offload_dir = \"/content/offload_dir\"\n",
        "\n",
        "# Create offload directory if it doesn't exist\n",
        "os.makedirs(offload_dir, exist_ok=True)\n",
        "\n",
        "# Load the model\n",
        "Mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=offload_dir\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "Mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ZLmoMphcWH"
      },
      "outputs": [],
      "source": [
        "def mistral_generate(prompt):\n",
        "    Mistral_model_inputs = Mistral_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Ensure the model inputs are on the same device as the model\n",
        "    Mistral_model_inputs = {k: v.to(device) for k, v in Mistral_model_inputs.items()}\n",
        "\n",
        "    # Generate text\n",
        "    Mistral_generated_ids = Mistral_model.generate(**Mistral_model_inputs, max_new_tokens=150, do_sample=True, temperature=1,top_k=50,top_p=0.9)\n",
        "\n",
        "    # Decode & return\n",
        "    output_Mistral = Mistral_tokenizer.batch_decode(Mistral_generated_ids, skip_special_tokens=True)[0]\n",
        "    return output_Mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEJdzsNJpydF"
      },
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qvrt8Mq83bg0"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "T5_model = \"google/flan-t5-large\"\n",
        "T5_tokenizer = T5Tokenizer.from_pretrained(T5_model)\n",
        "model = T5ForConditionalGeneration.from_pretrained(T5_model)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2qRmPly8YSD"
      },
      "outputs": [],
      "source": [
        "def T5_generate(prompt):\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    T5_inputs = T5_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate the output with group beam search and diversity penalty\n",
        "    outputs = model.generate(\n",
        "        T5_inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = T5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSO_dGGXrFVj"
      },
      "source": [
        "### Llama 3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pwtP2aCK_3TQ"
      },
      "outputs": [],
      "source": [
        "# Define the model ID and tokenizer\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Load the model and tokenizer with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,  # Enable 4-bit quantization\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtpBTy9m1mfb"
      },
      "outputs": [],
      "source": [
        "def llama_generate(prompt):\n",
        "    try:\n",
        "        # Generate text\n",
        "        outputs = pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=150,\n",
        "            eos_token_id=pipe.tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "        # Return the generated text\n",
        "        if outputs:\n",
        "            return outputs[0]['generated_text'][len(prompt):]\n",
        "        else:\n",
        "            return \"No output generated.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during text generation: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYX-XTm0joH3"
      },
      "source": [
        "### Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWF2VLb4f0mO"
      },
      "outputs": [],
      "source": [
        "def model_response(prompt, N, item_size, model_name):\n",
        "    res = []\n",
        "    function_name = None\n",
        "\n",
        "    if model_name == 'Mistral':\n",
        "        function_name = mistral_generate\n",
        "    elif model_name == 'Llama':\n",
        "        function_name = llama_generate\n",
        "    elif model_name == 'T5':\n",
        "        function_name = T5_generate\n",
        "    else:\n",
        "        print('Model name is not valid')\n",
        "        return\n",
        "\n",
        "    for i in range(N):\n",
        "        response = function_name(prompt)\n",
        "        res.append(response)\n",
        "        print(f'Response {i+1}:')\n",
        "        print(response)\n",
        "        print('------------------')\n",
        "\n",
        "\n",
        "    # Save results to a JSON file\n",
        "    output_file = f\"{model_name}_responses_{item_size}.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(res, f, indent=4)\n",
        "\n",
        "    files.download(output_file)\n",
        "    print(f\"Results saved to {output_file}\")\n",
        "\n",
        "    print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOoy14s2lMPM"
      },
      "source": [
        "### small item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qRLIVBGsk1k-"
      },
      "outputs": [],
      "source": [
        "model_response(formatted_small_zero, N, 'small', 'Mistral')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_small_one, N, 'small_one', 'Mistral')"
      ],
      "metadata": {
        "id": "OtO07cMlhuz2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2QHUodKalKni"
      },
      "outputs": [],
      "source": [
        "model_response(formatted_small_zero, N, 'small', 'Llama')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_small_one, N, 'small_one', 'Llama')"
      ],
      "metadata": {
        "id": "RrbzShgri_T8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J52gv5yDoD7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_response(formatted_small_zero, N, 'small', 'T5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_small_one, N, 'small_one', 'T5')"
      ],
      "metadata": {
        "id": "Cm1qFSNojBYg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_G5qt4lOJU"
      },
      "source": [
        "### medium item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LY5KCfSkk4JO"
      },
      "outputs": [],
      "source": [
        "model_response(formatted_medium_zero, N, 'medium','Mistral')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_medium_one, N, 'medium_one','Mistral')"
      ],
      "metadata": {
        "id": "JfMd3okal3ul",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YCVNdJ83lWOO"
      },
      "outputs": [],
      "source": [
        "model_response(formatted_medium_zero, N, 'medium', 'Llama')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_medium_one, N, 'medium_one', 'Llama')"
      ],
      "metadata": {
        "id": "zXDJWVxyl8OC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_medium_zero, N, 'medium', 'T5')"
      ],
      "metadata": {
        "id": "7XOs1h0EQbw4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_medium_one, N, 'medium_one', 'T5')"
      ],
      "metadata": {
        "id": "ISzNtWqFmANo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70eo5q_lle3I"
      },
      "source": [
        "### big item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Juc6av7dk4zt"
      },
      "outputs": [],
      "source": [
        "model_response(formatted_big_zero, N, 'big', 'Mistral')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_big_one, N, 'big_one', 'Mistral')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x2l7Jmm_mLiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3nisXZdXliDy"
      },
      "outputs": [],
      "source": [
        "model_response(formatted_big_zero, N, 'big', 'Llama')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_big_one, N, 'big_one', 'Llama')"
      ],
      "metadata": {
        "id": "I6K7KXbJmPX9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_big_zero, N, 'big', 'T5')"
      ],
      "metadata": {
        "id": "quQhiwMLVAnp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_big_one, N, 'big_one', 'T5')"
      ],
      "metadata": {
        "id": "_mmjZWIrmTAP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BqofY0WAKse"
      },
      "source": [
        "### giant item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPP44bUOtqIM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_response(formatted_giant_zero, N, 'giant', 'Mistral')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_giant_one, N, 'giant_one', 'Mistral')"
      ],
      "metadata": {
        "id": "LTNicVm1mb7Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UyLS1y8oLoz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_response(formatted_giant_zero, N, 'giant', 'Llama')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_giant_one, N, 'giant_one', 'Llama')"
      ],
      "metadata": {
        "id": "Gk5z_HLGmgb-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_giant_zero, N, 'giant', 'T5')"
      ],
      "metadata": {
        "id": "CUhW4GfhbAli",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_giant_one, N, 'giant_one', 'T5')"
      ],
      "metadata": {
        "id": "NlCxHAkDbETi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### items"
      ],
      "metadata": {
        "id": "0lDtj6GNich9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_dental_floss, N, 'dental_floss', 'Llama')"
      ],
      "metadata": {
        "id": "e5mriVKPWlt0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_speaker, N, 'speaker', 'Llama')"
      ],
      "metadata": {
        "id": "E96_eMOeiob_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_rolex, N, 'rolex', 'Llama')"
      ],
      "metadata": {
        "id": "dnjTN2qpit8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response(formatted_spaceship, N, 'spaceship', 'Llama')"
      ],
      "metadata": {
        "id": "YWlSSnJuixO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tfta3-fWDiBo",
        "pSO_dGGXrFVj",
        "pOoy14s2lMPM",
        "JM_G5qt4lOJU",
        "70eo5q_lle3I",
        "6BqofY0WAKse"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}